# -*- coding: utf-8 -*-
"""API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18aL2cVL3J5eT_VRUE52SiDrl_R6-96xx
"""

#Installing all requirements
!pip install langchain
!pip install -U langchain-community
!pip install sentence-transformers
!pip install faiss-cpu
!pip install groq
!pip install torchaudio
!pip install gTTS
!pip install soundfile
!pip install playsound
!pip install fastapi uvicorn

from huggingface_hub import notebook_login
notebook_login()

from google.colab import files
uploaded = files.upload()

import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline
from google.colab import userdata
from groq import Groq
from gtts import gTTS
from IPython.display import Audio
import os
from datetime import datetime

embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

client = Groq(
    api_key=userdata.get('GROQ_API_KEY')
)

asr = pipeline("automatic-speech-recognition", model="distil-whisper/distil-large-v2",device="cpu")

os.makedirs("recordings", exist_ok=True)


def format_data(data):
  df=pd.read_csv(data)
  df.head()
  formatted_data = []
  for index, row in df.iterrows():
    formatted_text = f"Disease: {row['Name']}. Symptoms: {row['Symptoms']}. Recommendations: {row['Treatments']}."
    formatted_data.append(formatted_text)
  return formatted_data


def convert_data_to_embeddings(data):
  embeddings = embedding_model.encode(data)
  return embeddings


def create_vector_store(embeddings):
  d = embeddings.shape[1]
  index = faiss.IndexFlatL2(d)
  index.add(embeddings)
  return index

def automatic_speech_recognition(patient_recording):
  patient_prompt_dict = asr(patient_recording)
  patient_prompt = patient_prompt_dict["text"]
  return patient_prompt

def retrieval(patient_prompt,index,formatted_data):
  query_embedding = embedding_model.encode([patient_prompt])
  k = 2
  distances, indices = index.search(query_embedding, k)
  retrieved_info = [formatted_data[idx] for idx in indices[0]]
  context = "\n".join(retrieved_info)
  return context


def LLM(context,patient_prompt):
  system_prompt = f"""You are an AI medical assistant. Based on the following information:{context} Generate a helpful and empathetic reply to the user query:
 '{patient_prompt}'.If you think the condition is serious, then immediately suggest the patient to call emergency or else no need.If you are recommending any
  medication, then mention to consult a doctor before taking it. Do ask the patient if they are experiencing any symptom that what type of symptom is it for
  example cough now your job is to ask the cough is dry or producing phlegm or sputum. You also have to ask since when they are experiencing this problem if
  they do not tell you. You will also ask if the symptom is triggered or worsen by any action or effect of anything, asking this helps in better diagnosis.
  You will also ask that if they are experiencing any other symptoms for example a patient says that he has pain in his stomach so will ask that if he is
  feeling nauseous or having fever, asking this also help in better diagnosis. But do not confuse the patient by asking too many questions ask only those
  questions which are highly relevant for the diagnosis."""
  chat_completion = client.chat.completions.create(
  messages=[
        {
            "role": "system",
            "content": system_prompt,
        },
        {
            "role":"user",
            "content": patient_prompt
        }
    ],
    model="llama-3.3-70b-versatile",
    )
  model_output=chat_completion.choices[0].message.content
  return model_output


def text_to_speech(model_output):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"recordings/result_{timestamp}.mp3"
    tts = gTTS(model_output)
    tts.save(filename)
    return filename

from fastapi import FastAPI,File,UploadFile
from pydantic import BaseModel
from fastapi.responses import FileResponse
from io import BytesIO

app=FastAPI()

os.makedirs("uploads", exist_ok=True)
os.makedirs("outputs", exist_ok=True)

file_content = uploaded['Diseases_Symptoms.csv']
formatted_data = format_data(BytesIO(file_content))

embeddings = convert_data_to_embeddings(formatted_data)
vector_store = create_vector_store(embeddings)


def save_uploaded_file(upload_file: UploadFile, save_as: str):
    with open(save_as, "wb") as f:
        content = upload_file.file.read()
        f.write(content)
    return save_as

@app.post("/upload-audio")
async def upload_audio(speech: UploadFile = File(...)):
    filename = f"uploads/{speech.filename}"
    save_uploaded_file(speech, filename)
    return {"filename": filename}


def process_audio(file_id: str, vector_store, formatted_data) -> str:
    patient_prompt = automatic_speech_recognition(file_id)  # Input = file path
    context = retrieval(patient_prompt, vector_store, formatted_data)
    model_output = LLM(context, patient_prompt)
    audio_path = text_to_speech(model_output)
    return audio_path  # path to generated audio file


@app.post("/get-audio")
async def get_audio(file_id: str):
    audio_file_path = process_audio(file_id, vector_store, formatted_data)
    return FileResponse(audio_file_path, media_type="audio/mpeg", filename="response.mp3")